{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Global seed set to 1024\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1024"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import torch.nn as nn\n",
    "from  torch.optim import Adam\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "from torch_geometric.nn import GATv2Conv\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from torch.utils.data import random_split\n",
    "\n",
    "import pytorch_lightning as pl\n",
    "from pytorch_lightning import Trainer, seed_everything\n",
    "from pytorch_lightning.loggers import TensorBoardLogger\n",
    "from pytorch_lightning.callbacks import ModelCheckpoint, EarlyStopping\n",
    "\n",
    "seed_everything(1024, workers=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TODO\n",
    "\n",
    "[ ] CPU -> GPU \n",
    "\n",
    "[ ] add trainer of pytorch-lightning\n",
    "\n",
    "[ ] Black and White assymentry\n",
    "\n",
    "[ ] Train and Valid set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[    0,     1,     2,  ...,  7410,   884,  6958],\n",
      "        [   56,   171,    65,  ...,  1125, 29994, 23078]])\n",
      "tensor([[0., 0., 1.],\n",
      "        [0., 0., 1.],\n",
      "        [0., 0., 2.],\n",
      "        ...,\n",
      "        [0., 0., 1.],\n",
      "        [1., 0., 0.],\n",
      "        [0., 0., 1.]])\n",
      "37393\n"
     ]
    }
   ],
   "source": [
    "###  Load data\n",
    "# edge_index        (2, E) -> (2, e)  batch\n",
    "# edge_attr         (E, 3) -> (e, 3)  batch\n",
    "# node_features     (N, X) -> (e, 2X) will be sampled at ChessModel\n",
    "\n",
    "data = torch.load('./data/graph_data.pt')\n",
    "edge_index = data['edge_index']\n",
    "edge_attr = data['edge_attr']\n",
    "num_node = data['num_node']\n",
    "\n",
    "print(edge_index)\n",
    "print(edge_attr)\n",
    "print(num_node)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GraphDataModule(pl.LightningDataModule):\n",
    "    def __init__(self, data_dir:str=\"path/to/dir\", proportion:list=[0.8, 0.1, 0.1], batch_size:int=32, num_workers:int=32):\n",
    "        super().__init__()\n",
    "        self.data_dir = data_dir\n",
    "        \n",
    "        self.batch_size = batch_size\n",
    "        self.num_workers = num_workers\n",
    "        \n",
    "        self.train_portion, self.val_portion, self.test_portion = proportion\n",
    "\n",
    "\n",
    "    def setup(self, stage=None):\n",
    "        data = torch.load(self.data_dir)\n",
    "        edge_index = data['edge_index']\n",
    "        edge_attr = data['edge_attr']\n",
    "\n",
    "        # size\n",
    "        dataset_size = len(edge_attr)\n",
    "        train_size = int(self.train_portion * dataset_size)\n",
    "        val_size = int(self.val_portion * dataset_size)\n",
    "        test_size = dataset_size - train_size - val_size\n",
    "        print(dataset_size, train_size, val_size)\n",
    "\n",
    "        # preprocessing\n",
    "        epsilon = 1e-8\n",
    "        edge_attr = edge_attr / (edge_attr.sum(dim=1, keepdim=True) + epsilon)\n",
    "        edge_index = edge_index.T\n",
    "        \n",
    "        # random split\n",
    "        self.data = TensorDataset(edge_index, edge_attr)\n",
    "        self.train_dataset, self.val_dataset, self.test_dataset = random_split(self.data, [train_size, val_size, test_size])\n",
    "\n",
    "    def train_dataloader(self):\n",
    "        return DataLoader(self.train_dataset, batch_size=self.batch_size, shuffle=True, num_workers=self.num_workers)\n",
    "\n",
    "    def val_dataloader(self):\n",
    "        return DataLoader(self.val_dataset, batch_size=self.batch_size, num_workers=self.num_workers)\n",
    "    \n",
    "    def test_dataloader(self):\n",
    "        return DataLoader(self.test_dataset, batch_size=self.batch_size, num_workers=self.num_workers)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 1956716])\n",
      "torch.Size([1956716, 4])\n"
     ]
    }
   ],
   "source": [
    "edge_index_doubled = edge_index.repeat(1, 2)\n",
    "\n",
    "ones = torch.ones_like(edge_attr[:,[0]])\n",
    "zeros = torch.zeros_like(edge_attr[:,[0]])\n",
    "edge_attr_WB = torch.concat([edge_attr, zeros], dim=1)\n",
    "edge_attr_BW = torch.concat([torch.flip(edge_attr, dims=[1]), ones], dim=1)\n",
    "edge_attr_doubled = torch.concat([edge_attr_WB, edge_attr_BW], dim=0)\n",
    "\n",
    "print(edge_index_doubled.shape)\n",
    "print(edge_attr_doubled.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "978358 782686 97835\n"
     ]
    }
   ],
   "source": [
    "myData = GraphDataModule(data_dir='./data/graph_data.pt', batch_size=32, num_workers=32)\n",
    "myData.prepare_data()\n",
    "myData.setup(stage=\"fit\")\n",
    "train_dataloader = myData.train_dataloader()\n",
    "val_dataloader = myData.val_dataloader()\n",
    "test_dataloader = myData.test_dataloader()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    def __init__(self, in_channels, hidden_channels, out_channels, num_node, num_layers, heads=1):\n",
    "        super().__init__()\n",
    "        \n",
    "        x = torch.randn(num_node, in_channels)\n",
    "        self.register_buffer('x', x)\n",
    "        \n",
    "        self.num_node = num_node\n",
    "        self.in_channels =in_channels\n",
    "        self.convs = nn.ModuleList()\n",
    "        self.convs.append(GATv2Conv(in_channels, hidden_channels, heads=heads, edge_dim=4))\n",
    "        for _ in range(num_layers - 2):\n",
    "            self.convs.append(GATv2Conv(hidden_channels, hidden_channels, heads=heads, edge_dim=4))\n",
    "        self.convs.append(GATv2Conv(hidden_channels * heads, out_channels, heads=1, edge_dim=4))\n",
    "\n",
    "\n",
    "    def forward(self, edge_index, edge_attr):\n",
    "        x = self.x\n",
    "        for i, conv in enumerate(self.convs):\n",
    "            x = conv(x, edge_index, edge_attr)\n",
    "            if i != len(self.convs) - 1:\n",
    "                x = F.relu(x)\n",
    "                x = F.dropout(x, p=0.5, training=self.training)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(nn.Module):\n",
    "    def __init__(self, in_channels, hidden_channels, out_channels=3):\n",
    "        super().__init__()\n",
    "        hidden1, hidden2 = hidden_channels\n",
    "        self.Fc1 = nn.Linear(in_channels, hidden1, bias=False)\n",
    "        self.Fc2 = nn.Linear(hidden1, hidden2, bias=False)\n",
    "        self.Fc3 = nn.Linear(hidden2, out_channels, bias=False)\n",
    "\n",
    "    def forward(self, x):\n",
    "        h = F.relu(self.Fc1(x))\n",
    "        h = F.relu(self.Fc2(h))\n",
    "        output = self.Fc3(h)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ChessModel(pl.LightningModule):\n",
    "  def __init__(self, node_feature_dim, num_layers, heads, num_node, edge_index_doubled, edge_attr_doubled, learning_rate=1e-6):\n",
    "    \n",
    "    super().__init__()\n",
    "    self.edge_index_doubled = edge_index_doubled.to(self.device)\n",
    "    self.edge_attr_doubled = edge_attr_doubled.to(self.device)\n",
    "    self.save_hyperparameters(\"learning_rate\")\n",
    "\n",
    "    # init layers\n",
    "    self.Encoder = Encoder(node_feature_dim, node_feature_dim, node_feature_dim, num_node, num_layers, heads)\n",
    "    self.Decoder = Decoder(2 * node_feature_dim, [100, 100], 3)\n",
    "    \n",
    "    # init parameters reccursively\n",
    "    self.apply(self._init_parameters)\n",
    "\n",
    "  def _init_parameters(self, module):\n",
    "    if isinstance(module, nn.Linear) or isinstance(module, nn.LayerNorm):\n",
    "      nn.init.xavier_normal_(module.weight)\n",
    "      if module.bias is not None:\n",
    "        nn.init.zeros_(module.bias)\n",
    "\n",
    "  #### forward pass ###################################################\n",
    "  def _shared_step(self, batch, edge_index_doubled, edge_attr_doubled):\n",
    "    # 0. unpack batch\n",
    "    batch_edge_index, batch_edge_attr = batch\n",
    "    batch_edge_index = batch_edge_index.T\n",
    "\n",
    "    edge_index_doubled = edge_index_doubled.to(self.device)\n",
    "    edge_attr_doubled = edge_attr_doubled.to(self.device)\n",
    "\n",
    "    # 1. Forward pass through Encoder\n",
    "    node_features = self.Encoder(edge_index_doubled, edge_attr_doubled)\n",
    "   \n",
    "    # 2. Sample node features\n",
    "    sampled_node_features = torch.cat([node_features[batch_edge_index[0]], node_features[batch_edge_index[1]]], dim=-1)\n",
    "\n",
    "    # 3. Forward pass through Decoder\n",
    "    outputs = self.Decoder(sampled_node_features)\n",
    "    \n",
    "    # 4. Set targets\n",
    "    targets = batch_edge_attr\n",
    "\n",
    "    loss = F.mse_loss(outputs, targets)\n",
    "    return loss\n",
    "\n",
    "  def training_step(self, batch, batch_idx):\n",
    "    loss = self._shared_step(batch, self.edge_index_doubled, self.edge_attr_doubled)\n",
    "    self.log('train_loss', loss)\n",
    "    return loss\n",
    "\n",
    "  def validation_step(self, batch, batch_idx):\n",
    "    loss = self._shared_step(batch, self.edge_index_doubled, self.edge_attr_doubled)\n",
    "    self.log('val_loss', loss)\n",
    "\n",
    "  def predict_step(self, batch, batch_idx):\n",
    "    loss = self._shared_step(batch, self.edge_index_doubled, self.edge_attr_doubled)\n",
    "    self.log('pred_loss', loss)\n",
    "  #####################################################################\n",
    "\n",
    "  #### configure optimizer\n",
    "  def configure_optimizers(self):\n",
    "    optimizer = Adam(self.parameters(), lr=self.hparams.learning_rate)\n",
    "    return {\n",
    "      \"optimizer\": optimizer,\n",
    "      \"lr_scheduler\": {\n",
    "        \"scheduler\": ReduceLROnPlateau(optimizer, mode='min'),\n",
    "        \"monitor\": \"val_loss\",\n",
    "      },\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # module\n",
    "# myChess = ChessModel(node_feature_dim=10, num_layers=2, heads=3, \n",
    "#                      num_node=num_node, \n",
    "#                      edge_index_doubled=edge_index_doubled, \n",
    "#                      edge_attr_doubled=edge_attr_doubled)\n",
    "\n",
    "# # trainer\n",
    "# logger = TensorBoardLogger(\"tb_logs\", name=\"test5\")\n",
    "# callbacks = [ModelCheckpoint(monitor=\"val_loss\", mode=\"min\"),\n",
    "#              EarlyStopping(monitor=\"val_loss\", mode=\"min\", patience=3)]\n",
    "# trainer = Trainer(logger=logger, callbacks=[], val_check_interval=0.1, max_epochs=1000)\n",
    "# trainer.fit(myChess, train_dataloader, val_dataloader)\n",
    "# trainer.predict(myChess, test_dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([978358, 2])\n",
      "torch.Size([978358, 3])\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[12], line 7\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[39mprint\u001b[39m(test_edge_attr\u001b[39m.\u001b[39mshape)\n\u001b[1;32m      6\u001b[0m elo \u001b[39m=\u001b[39m ELO(\u001b[39m\"\u001b[39m\u001b[39m./data/temp.csv\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m----> 7\u001b[0m elo\u001b[39m.\u001b[39;49mtrain(test_edge_index)\n\u001b[1;32m      8\u001b[0m outputs \u001b[39m=\u001b[39m elo\u001b[39m.\u001b[39mpredict(test_edge_index)\n\u001b[1;32m      9\u001b[0m \u001b[39mprint\u001b[39m(outputs\u001b[39m.\u001b[39mshape)\n",
      "File \u001b[0;32m~/dongwook_gnn/elo.py:15\u001b[0m, in \u001b[0;36mELO.train\u001b[0;34m(self, test_edge_index)\u001b[0m\n\u001b[1;32m     13\u001b[0m white \u001b[39m=\u001b[39m row[\u001b[39m1\u001b[39m]\n\u001b[1;32m     14\u001b[0m black \u001b[39m=\u001b[39m row[\u001b[39m2\u001b[39m]\n\u001b[0;32m---> 15\u001b[0m \u001b[39mif\u001b[39;00m [white, black] \u001b[39min\u001b[39;00m test_edge_index\u001b[39m.\u001b[39;49mtolist():\n\u001b[1;32m     16\u001b[0m \t\u001b[39mcontinue\u001b[39;00m\n\u001b[1;32m     17\u001b[0m result \u001b[39m=\u001b[39m \u001b[39mfloat\u001b[39m(row[\u001b[39m3\u001b[39m])\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from elo import ELO\n",
    "\n",
    "test_edge_index, test_edge_attr= myData.test_dataset.dataset.tensors\n",
    "print(test_edge_index.shape)\n",
    "print(test_edge_attr.shape)\n",
    "elo = ELO(\"./data/temp.csv\")\n",
    "elo.train(test_edge_index)\n",
    "outputs = elo.predict(test_edge_index)\n",
    "print(outputs.shape)\n",
    "loss = F.mse_loss(outputs, test_edge_attr)\n",
    "print(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
